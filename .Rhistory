pi_R = pi_R, hat_pi_R = hat_pi_R, num_post = num_post,
frame_B = frame_B, frame_R = frame_R, trim_method = trim_method,
trim_c = trim_c, D = D, parallel = parallel, n_cores = n_cores,
wts_adj = wts_adj, adjust = adjust, tol = tol,
num_reps = num_reps, run_adapt = run_adapt,
K_max = K_max, adapt_seed = adapt_seed, K_fixed = NULL,
fixed_seed = fixed_seed, class_cutoff = 0.05,
n_runs = n_runs, burn = burn, thin = thin, update = update,
save_res = FALSE, save_path = save_path)
### Read in population and sample data
load(paste0(wd, data_dir, "scen_", scenario, "/sim_pop_wolcan.RData"))
load(paste0(wd, data_dir, "scen_", scenario, "/sim_samp_", samp_i, "_B_wolcan.RData"))
load(paste0(wd, data_dir, "scen_", scenario, "/sim_samp_", samp_i, "_R_wolcan.RData"))
### Run weighted model
res <- wolcan(x_mat = x_mat, dat_B = dat_B, dat_R = dat_R,
pred_covs_B = pred_covs_B, pred_covs_R = pred_covs_R,
pi_R = pi_R, hat_pi_R = hat_pi_R, num_post = num_post,
frame_B = frame_B, frame_R = frame_R, trim_method = trim_method,
trim_c = trim_c, D = D, parallel = parallel, n_cores = n_cores,
wts_adj = wts_adj, adjust = adjust, tol = tol,
num_reps = num_reps, run_adapt = run_adapt,
K_max = K_max, adapt_seed = adapt_seed, K_fixed = NULL,
fixed_seed = fixed_seed, class_cutoff = 0.05,
n_runs = n_runs, burn = burn, thin = thin, update = update,
save_res = FALSE, save_path = save_path)
pred_covs_R
scenario
load("~/Documents/GitHub/WOLCAN/Data/scen_1/sim_samp_1_B_wolcan.RData")
### Define data and parameters
# Estimating pseudo-weights
x_mat <- sim_samp_B$X_data  # Multivariate categorical variables
dat_B <- data.frame(sim_samp_B$covs)  # Covariates for NPS
dat_R <- data.frame(sim_samp_R$covs)  # Covariates for reference
### Read in population and sample data
load(paste0(wd, data_dir, "scen_", scenario, "/sim_pop_wolcan.RData"))
load(paste0(wd, data_dir, "scen_", scenario, "/sim_samp_", samp_i, "_B_wolcan.RData"))
load(paste0(wd, data_dir, "scen_", scenario, "/sim_samp_", samp_i, "_R_wolcan.RData"))
### Define data and parameters
# Estimating pseudo-weights
x_mat <- sim_samp_B$X_data  # Multivariate categorical variables
dat_B <- data.frame(sim_samp_B$covs)  # Covariates for NPS
dat_R <- data.frame(sim_samp_R$covs)  # Covariates for reference
pred_covs_B <- c("A1", "A2", "A1A2", "A3")  # Covariates to predict NPS selection
pred_covs_R <- c("A1", "A2", "A1A2", "A3")  # Covariates to predict RS selection
pi_R <- sim_samp_R$pi_R  # RS selection probabilites for those in RS
hat_pi_R <- NULL  # RS selection probabilities for those in NPS
num_post <- 1000  # Number of posterior BART draws for estimating weights
frame_B <- 1  # Coverage probability of NPS frame
frame_R <- 1  # Coverage probability of RS frame
trim_method = "t2"  # Trimming method using IQR
trim_c = 20         # Trimming constant
# Model estimation
D <- 20            # Number of sets of MI pseudo-weights
parallel <- TRUE   # Whether to parallelize for MI
n_cores <- 8       # Number of cores to use for parallelization
wts_adj <- "MI"    # Adjustment method to account for weights uncertainty
MI <- TRUE         # Whether to run MI procedure for variance estimation
adjust <- TRUE     # Whether to adjust variance for pseudo-likelihood
tol <- 1e-8        # Underflow tolerance
run_adapt <- TRUE  # Whether to run adaptive sampler to get K
K_max <- 30          # Max number of latent classes for adaptive sampler
adapt_seed <- samp_i # Seed for adaptive sampler
fixed_seed <- samp_i # Seed for fixed sampler
# Set directories
wd <- "~/Documents/GitHub/WOLCAN/"  # Working directory
data_dir <- "Data/"    # Data directory
res_dir <- "Results/"  # Results directory
code_dir <- "Model_Code/"  # Model code directory
# Create scenario results folder if it doesn't exist
dir_path <- paste0(wd, res_dir, "scen_", scenario, "/")
if (!dir.exists(dir_path)) {
dir.create(file.path(dir_path))
}
# Define path to save results
save_path <- paste0(wd, res_dir, "scen_", scenario, "/samp_", samp_i)
# Check if results already exist
already_done <- file.exists(paste0(save_path, "_wolcan_results.RData"))
### Run weighted model
res <- wolcan(x_mat = x_mat, dat_B = dat_B, dat_R = dat_R,
pred_covs_B = pred_covs_B, pred_covs_R = pred_covs_R,
pi_R = pi_R, hat_pi_R = hat_pi_R, num_post = num_post,
frame_B = frame_B, frame_R = frame_R, trim_method = trim_method,
trim_c = trim_c, D = D, parallel = parallel, n_cores = n_cores,
wts_adj = wts_adj, adjust = adjust, tol = tol,
num_reps = num_reps, run_adapt = run_adapt,
K_max = K_max, adapt_seed = adapt_seed, K_fixed = NULL,
fixed_seed = fixed_seed, class_cutoff = 0.05,
n_runs = n_runs, burn = burn, thin = thin, update = update,
save_res = FALSE, save_path = save_path)
wts_adj = "WS all"
### Run weighted model
res <- wolcan(x_mat = x_mat, dat_B = dat_B, dat_R = dat_R,
pred_covs_B = pred_covs_B, pred_covs_R = pred_covs_R,
pi_R = pi_R, hat_pi_R = hat_pi_R, num_post = num_post,
frame_B = frame_B, frame_R = frame_R, trim_method = trim_method,
trim_c = trim_c, D = D, parallel = parallel, n_cores = n_cores,
wts_adj = wts_adj, adjust = adjust, tol = tol,
num_reps = num_reps, run_adapt = run_adapt,
K_max = K_max, adapt_seed = adapt_seed, K_fixed = NULL,
fixed_seed = fixed_seed, class_cutoff = 0.05,
n_runs = n_runs, burn = burn, thin = thin, update = update,
save_res = FALSE, save_path = save_path)
# Source R model functions
source(paste0(wd, code_dir, "model_functions.R"))
### Run weighted model
res <- wolcan(x_mat = x_mat, dat_B = dat_B, dat_R = dat_R,
pred_covs_B = pred_covs_B, pred_covs_R = pred_covs_R,
pi_R = pi_R, hat_pi_R = hat_pi_R, num_post = num_post,
frame_B = frame_B, frame_R = frame_R, trim_method = trim_method,
trim_c = trim_c, D = D, parallel = parallel, n_cores = n_cores,
wts_adj = wts_adj, adjust = adjust, tol = tol,
num_reps = num_reps, run_adapt = run_adapt,
K_max = K_max, adapt_seed = adapt_seed, K_fixed = NULL,
fixed_seed = fixed_seed, class_cutoff = 0.05,
n_runs = n_runs, burn = burn, thin = thin, update = update,
save_res = FALSE, save_path = save_path)
num_reps <- 100    # Number of bootstrap replicates for WS adjustment
### Run weighted model
res <- wolcan(x_mat = x_mat, dat_B = dat_B, dat_R = dat_R,
pred_covs_B = pred_covs_B, pred_covs_R = pred_covs_R,
pi_R = pi_R, hat_pi_R = hat_pi_R, num_post = num_post,
frame_B = frame_B, frame_R = frame_R, trim_method = trim_method,
trim_c = trim_c, D = D, parallel = parallel, n_cores = n_cores,
wts_adj = wts_adj, adjust = adjust, tol = tol,
num_reps = num_reps, run_adapt = run_adapt,
K_max = K_max, adapt_seed = adapt_seed, K_fixed = NULL,
fixed_seed = fixed_seed, class_cutoff = 0.05,
n_runs = n_runs, burn = burn, thin = thin, update = update,
save_res = TRUE, save_path = save_path)
### Run weighted model
res <- wolcan(x_mat = x_mat, dat_B = dat_B, dat_R = dat_R,
pred_covs_B = pred_covs_B, pred_covs_R = pred_covs_R,
pi_R = pi_R, hat_pi_R = hat_pi_R, num_post = num_post,
frame_B = frame_B, frame_R = frame_R, trim_method = trim_method,
trim_c = trim_c, D = D, parallel = parallel, n_cores = n_cores,
wts_adj = wts_adj, adjust = adjust, tol = tol,
num_reps = num_reps, run_adapt = run_adapt,
K_max = K_max, adapt_seed = adapt_seed, K_fixed = NULL,
fixed_seed = fixed_seed, class_cutoff = 0.05,
n_runs = n_runs, burn = burn, thin = thin, update = update,
save_res = FALSE, save_path = save_path)
res$estimates_adjust$pi_med
dim(res$estimates_adjust$pi_red)
D
dim(res$estimates$pi_red)
wts_adj = "WS mean"
### Run weighted model
res <- wolcan(x_mat = x_mat, dat_B = dat_B, dat_R = dat_R,
pred_covs_B = pred_covs_B, pred_covs_R = pred_covs_R,
pi_R = pi_R, hat_pi_R = hat_pi_R, num_post = num_post,
frame_B = frame_B, frame_R = frame_R, trim_method = trim_method,
trim_c = trim_c, D = D, parallel = parallel, n_cores = n_cores,
wts_adj = wts_adj, adjust = adjust, tol = tol,
num_reps = num_reps, run_adapt = run_adapt,
K_max = K_max, adapt_seed = adapt_seed, K_fixed = NULL,
fixed_seed = fixed_seed, class_cutoff = 0.05,
n_runs = n_runs, burn = burn, thin = thin, update = update,
save_res = FALSE, save_path = save_path)
wts_adj = MI
wts_adj = "MI"
adjust = FALSE
### Run weighted model
res <- wolcan(x_mat = x_mat, dat_B = dat_B, dat_R = dat_R,
pred_covs_B = pred_covs_B, pred_covs_R = pred_covs_R,
pi_R = pi_R, hat_pi_R = hat_pi_R, num_post = num_post,
frame_B = frame_B, frame_R = frame_R, trim_method = trim_method,
trim_c = trim_c, D = D, parallel = parallel, n_cores = n_cores,
wts_adj = wts_adj, adjust = adjust, tol = tol,
num_reps = num_reps, run_adapt = run_adapt,
K_max = K_max, adapt_seed = adapt_seed, K_fixed = NULL,
fixed_seed = fixed_seed, class_cutoff = 0.05,
n_runs = n_runs, burn = burn, thin = thin, update = update,
save_res = FALSE, save_path = save_path)
res$estimates_adjust$pi_med
dim(res$estimates_adjust$pi_red)
wts_adj = "none"
### Run weighted model
res <- wolcan(x_mat = x_mat, dat_B = dat_B, dat_R = dat_R,
pred_covs_B = pred_covs_B, pred_covs_R = pred_covs_R,
pi_R = pi_R, hat_pi_R = hat_pi_R, num_post = num_post,
frame_B = frame_B, frame_R = frame_R, trim_method = trim_method,
trim_c = trim_c, D = D, parallel = parallel, n_cores = n_cores,
wts_adj = wts_adj, adjust = adjust, tol = tol,
num_reps = num_reps, run_adapt = run_adapt,
K_max = K_max, adapt_seed = adapt_seed, K_fixed = NULL,
fixed_seed = fixed_seed, class_cutoff = 0.05,
n_runs = n_runs, burn = burn, thin = thin, update = update,
save_res = FALSE, save_path = save_path)
#=================== Generate population: SCENARIO 0 ===========================
# Variance adjustment, pi_R unknown and predicted using continuous BART,
# sample size 2000/40,000 (5%), non-overlapping patterns, X~S,
# high overlap for pi_R and pi_B
scenario <- 0
# Create data folder if it doesn't exist
dir_path <- paste0(wd, data_dir, "scen_", scenario, "/")
if (!dir.exists(dir_path)) {
dir.create(file.path(dir_path))
}
### General parameters
rho <- 0.5     # Correlation between selection variables A1 and A2
N <- 40000     # Population size
pop_seed <- 1  # Set seed
### Parameters for generating categorical latent class assignment C
formula_c <- "~ A1 + A2 + A3"
res$estimates$pi_med
prop.table(table(sim_samp_B$c_all))
prop.table(table(sim_pop$c_all))
### General parameters
rho <- 0.5     # Correlation between selection variables A1 and A2
N <- 40000     # Population size
pop_seed <- 1  # Set seed
### Parameters for generating categorical latent class assignment C
formula_c <- "~ A1 + A2 + A3"
beta_mat_c <- matrix(c(0, 0, 0, 0,
0.4, -0.5, 0.75, 0.1,
-0.2, -1.5, 1.2, 0.25), nrow = 3, byrow = TRUE)
colnames(beta_mat_c) <- c("Intercept", "A1", "A2", "A3")
### Parameters for generating observed manifest variables X
J <- 30; R <- 4; K <- 3
formula_x <- "~ c_all"
V_unique <- data.frame(c_all = as.factor(1:K))
profiles <- as.matrix(data.frame(C1 = c(rep(1, times = 0.5 * J),
rep(3, times = 0.5 * J)),
C2 = c(rep(4, times = 0.2 * J),
rep(2, times = 0.8 * J)),
C3 = c(rep(3, times = 0.3 * J),
rep(4, times = 0.4 * J),
rep(1, times = 0.3 * J))))
modal_theta_prob <- 0.85
beta_list_x_temp <- get_betas_x(profiles = profiles, R = R,
modal_theta_prob = modal_theta_prob,
formula_x = formula_x, V_unique = V_unique)
# Add in coefficients for A3, updating formula_x and beta_list_x
formula_x <- "~ c_all + A1 + A3"
# Items 1-2 are affected in the following manner:
# level 2 probability increases as A3 increases
beta_list_x <- lapply(1:2, function(j) cbind(beta_list_x_temp[[j]],
A1 = rep(0, 4),
A3 = c(0, 0.5, 0, 0)))
beta_list_x <- c(beta_list_x, lapply(3:(J-2), function(j)
cbind(beta_list_x_temp[[j]], A1 = rep(0, 4), A3 = rep(0, 4))))
# Items 29-30 are affected as follows: level 2 probability decrease with A1
beta_list_x <- c(beta_list_x, lapply((J-1):J, function(j)
cbind(beta_list_x_temp[[j]], A1 = c(0, 1, 0, 0), A3 = rep(0, 4))))
V_unique <- as.data.frame(expand.grid(c_all = as.factor(1:K),
A1 = c(-4, 0, 4),
A3 = c(-8, 0, 8)))
round(get_categ_probs(beta_mat = beta_list_x[[1]], formula = formula_x,
V_unique = V_unique), 3)
### Generate population
n_B <- 2000  # Sample size for non-probability sample
n_R <- 2000  # Sample size for reference sample
sim_pop <- sim_pop_wolcan(N = N, J = J, K = K, R = R, rho = rho, n_B = n_B,
n_R = n_R, high_overlap = TRUE, formula_c = formula_c,
beta_mat_c = beta_mat_c, formula_x = formula_x,
beta_list_x = beta_list_x, pop_seed = pop_seed,
save_res = TRUE, save_path = dir_path)
# Latent class assignment correlations with selection covariates
cor(as.numeric(sim_pop$c_all), sim_pop$pop$A1)
plot(sim_pop$c_all, sim_pop$pop$A1)
scenario
num_samps <- 1
### Parameters
n_B <- 2000  # Sample size for non-probability sample
n_R <- 2000  # Sample size for reference sample
for (i in 1:num_samps) {
sim_samps <- sim_samp_wolcan(i = i, sim_pop = sim_pop, n_B = n_B, n_R = n_R,
scenario = scenario, save_res = TRUE, samp_seed = i,
save_path = dir_path)
}
load("~/Documents/GitHub/WOLCAN/Data/scen_0/sim_samp_1_B_wolcan.RData")
prop.table(table(sim_samp_B$c_all))
rm(list = ls())
library(tidyverse)  # data wrangling
# Directories
wd <- "~/Documents/GitHub/WOLCAN/"  # Working directory
# wd <- "/n/holyscratch01/stephenson_lab/Users/stephwu18/WOLCAN/"
data_dir <- "Application/Data_Processing"  # Raw data directory
res_dir <- "Application/"                  # Cleaned data directory
code_dir <- "Application/Data_Processing"  # Code directory
# Read in PRCS person-level data
prcs_p <- read.csv(paste0(wd, data_dir, "psam_p72.csv"))
data_dir <- "Application/Data_Processing/"  # Raw data directory
res_dir <- "Application/"                  # Cleaned data directory
code_dir <- "Application/Data_Processing/"  # Code directory
# Read in PRCS person-level data
prcs_p <- read.csv(paste0(wd, data_dir, "psam_p72.csv"))
# Read in PRCS household-level data
prcs_h <- read.csv(paste(wd, data_dir, "psam_h72.csv"))
# Read in PRCS household-level data
prcs_h <- read.csv(paste0(wd, data_dir, "psam_h72.csv"))
colnames(prcs_p)
View(prcs_p)
View(prcs_h)
size(prcs_p)
dim(prcs_p)
dims(prcs_h)
dim(prcs_h)
unique(prcs_p$SEX)
summary(prcs_p$AGEP)
?case_match
head(prcs_p$SCHL)
typeof(prcs_p$SCHL)
unique(prcs_p$SCHL)
summary(prcs_p$PINCP)
summary(prcs_p$FPINCP)
table(prcs_p$FPINCP)
table(prcs_p$FAGEP)
summary(prcsP$FAGEP)
summary(prcs_p$FAGEP)
summary(prcs_p$AGEP)
summary(prcs_p$PINCP)
table(prcs_p$FPINCP, useNA = "always")
dim(prcs_p$PINCP)
dim(prcs_p)
sum(is.na(prcs_p$PINCP) & (prcs_p$FPINCP == 1))
sum(is.na(prcs_p$PINCP) & (prcs_p$FPINCP == 0))
summary(prcs_h$HINCP)
table(prcs_h$FHINCP, useNA = "always")
dim(prcs_h)
library(readxl)     # read excel files
# Read in metropolitan areas
metro <- read_xlsx(paste0(wd, data_dir, "MSA2013_PUMA2020_crosswalk.xlsx"))
View(metro)
# Merge in metropolitan area information
prcs_metro <- prcs_h %>%
left_join(metro, by = join_by(PUMA20 == `PUMA Code`))
unique(prcs_h$PUMA20)
unique(metro$`PUMA Code`)
# Restric to PR
metro <- metro %>% filter(`State Name` == "Puerto Rico")
unique(prcs_h$PUMA20)
unique(metro$`PUMA Code`)
# Restrict to PR and convert metro PUMA code to numeric
metro <- metro %>%
filter(`State Name` == "Puerto Rico") %>%
mutate(PUMA20 = as.numeric(`PUMA Code`))
unique(metro$PUMA20)
### Merge in metropolitan area information
# Check no missing 2020 PUMAs
setdiff(unique(prcs_h$PUMA20), unique(metro$PUMA20))
# Merge in metro area info
prcs_metro <- prcs_h %>%
left_join(metro, by = join_by(PUMA20))
prcs_h[61513,]
# Merge in metro area info
prcs_metro <- prcs_h %>%
left_join(metro, by = join_by(PUMA20), relationship = "many-to-one")
prcs_h[61510:61520, 1:20]
# Merge in metro area info
prcs_metro <- prcs_h %>%
left_join(metro, by = join_by(PUMA20), relationship = "many-to-one",
multiple = "first")  # set this to match PUMA 700 with MSA 25020
# How many PUMAs are MSAs
mean(!is.na(prcs_metro$`MSA Code`))
# Read in metropolitan areas
metro <- read_xlsx(paste0(wd, data_dir, "MSA2013_PUMA2020_crosswalk.xlsx"))
View(metro)
length(unique(metro$`MSA Code`))
View(prcs_metro)
# Create urban variable
prcs_metro <- prcs_metro %>%
mutate(urban = ifelse(!is.na(`MSA Code`), 1, 0))
# Merge in metro area info
prcs_metro <- prcs_h %>%
left_join(metro, by = join_by(PUMA20), relationship = "many-to-one",
multiple = "first")  # set this to match PUMA 700 with MSA 25020
# How many PUMAs are MSAs: around 22%
mean(!is.na(prcs_metro$`MSA Code`))
# Read in metropolitan areas
metro <- read_xlsx(paste0(wd, data_dir, "MSA2013_PUMA2020_crosswalk.xlsx"))
# Restrict to PR and convert metro PUMA code to numeric
metro <- metro %>%
filter(`State Name` == "Puerto Rico") %>%
mutate(PUMA20 = as.numeric(`PUMA Code`))
### Merge in metropolitan area information
# Check no missing 2020 PUMAs other than group quarters (-9)
setdiff(unique(prcs_h$PUMA20), unique(metro$PUMA20))
# Merge in metro area info
prcs_metro <- prcs_h %>%
left_join(metro, by = join_by(PUMA20), relationship = "many-to-one",
multiple = "first")  # set this to match PUMA 700 with MSA 25020
# How many PUMAs are MSAs: around 22%
mean(!is.na(prcs_metro$`MSA Code`))
# Create urban variable
prcs_metro <- prcs_metro %>%
mutate(Urban = ifelse(!is.na(`MSA Code`), 1, 0))
### Add in annual household income
# Recode annual household income
prcs_h <- prcs_h %>%
mutate(Inc_hh = case_when(
HINCP <= 10000 ~ 1,  # 0-10k
HINCP > 10000 & HINCP <= 20000 ~ 2, # 10-20k
HINCP > 20000 ~ 3, # >20k
.default = HINCP
))
unique(prcs_h$Inc_hh)
View(prcs_h)
temp <- prcs_h$HINCP
unique(temp)
summary(temp)
### Obtain variables to be used to model selection
# Age, sex, rural residency, education, individual income, weights
prcs_vars <- prcs_p %>%
select(SERIALNO, SPORDER, AGEP, SEX, SCHL, PINCP, PWGTP) %>%
mutate(
Sex = factor(SEX, levels = c(1, 2), labels = c("M", "F")),
Educ = case_when(
SCHL %in% c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11) ~ "<=8th grade",
SCHL %in% c(12, 13, 14, 15, 16, 17) ~ "9th grade to GED",
SCHL %in% c(18, 19, 20, 21) ~ "some college or bachelors",
SCHL %in% c(22, 23, 24) ~ "graduate degree",
.default = SCHL)) %>%
rename(
Age = AGEP,
Income_indiv = PINCP,
Weight = PWGTP
)
### Obtain variables to be used to model selection
# Age, sex, rural residency, education, individual income, weights
prcs_vars <- prcs_p %>%
select(SERIALNO, SPORDER, AGEP, SEX, SCHL, PINCP, PWGTP) %>%
mutate(
Sex = factor(SEX, levels = c(1, 2), labels = c("M", "F")),
Educ = case_when(
SCHL %in% c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11) ~ 0, # "<=8th grade",
SCHL %in% c(12, 13, 14, 15, 16, 17) ~ 1, # "9th grade to GED",
SCHL %in% c(18, 19, 20, 21) ~ 2, #"some college or bachelors",
SCHL %in% c(22, 23, 24) ~ 3, #"graduate degree",
.default = SCHL)) %>%
rename(
Age = AGEP,
Income_indiv = PINCP,
Weight = PWGTP
)
### Add in annual household income
# Recode annual household income
prcs_h <- prcs_h %>%
mutate(Inc_hh = case_when(
HINCP <= 10000 ~ 1,  # 0-10k
HINCP > 10000 & HINCP <= 20000 ~ 2, # 10-20k
HINCP > 20000 ~ 3, # >20k
.default = HINCP
))
prcs_vars <- prcs_vars %>%
left_join(prcs_h %>% select(SERIALNO, Inc_hh),
by = join_by(SERIALNO))
### Add in urban variable
prcs_vars <- prcs_vars %>%
left_join(prcs_metro %>% select(SERIALNO, Urban),
by = join_by(SERIALNO))
# Number of individuals with complete data on selected variables
prcs_comp <- prcs_vars %>% drop_na()
summary(prcs_vars$Weight)
132642-113229
113229/3300000
113229/3200000
2000/3200000
?write.csv
# Save PRCS sample data
write.csv(prcs_comp, file = paste0(wd, res_dir, "prcs.csv"))
View(prcs_comp)
colnames(prcs_comp)
# Reorder variables
prcs_comp <- prcs_comp %>% select(SERIALNO, SPORDER, Weight, Age, Sex, Educ,
Inc_hh, Urban, Income_indiv)
### Save PRCS sample data
write.csv(prcs_comp, file = paste0(wd, res_dir, "prcs.csv"))
0.000625*40000
100000/3000000
3000000*0.035
3000000*0.0006
2000*0.0006
2000/0.0006
2000/3200000
100000/3200000
3200000/0.065
3200000*0.035
?lm
confint()
?confint
data_scen <- list(0, 8, 13, 14, 10, 19, 20, 21)
model_scen <- list("logistic", "bart_bin", "bart_500", "bart_1000", "bart_2000",
"bart_1000_cov")
# Total number of scenarios
num_scen <- length(model_scen) * length(data_scen)
weights_res <- matrix(NA, nrow = num_scen, ncol = num_samps)
num_samps <- 100
weights_res <- matrix(NA, nrow = num_scen, ncol = num_samps)
weights_res$scenario <- rep(data_scen, each = length(model_scen))
weights_res$model <- rep(model_scen, times = length(data_scen))
View(weights_res)
weights_res <- as.data.frame(matrix(NA, nrow = num_scen, ncol = num_samps))
weights_res$scenario <- rep(data_scen, each = length(model_scen))
weights_res$model <- rep(model_scen, times = length(data_scen))
num_scen
weights_res <- as.data.frame(matrix(1:25, nrow = num_scen, ncol = num_samps))
weights_res_mean <- rowMeans(weights_res)
weights_res_mean$scenario <- rep(data_scen, each = length(model_scen))
weights_res_mean$model <- rep(model_scen, times = length(data_scen))
weights_res_mean <- rowMeans(weights_res)
is.data.frame(weights_res_nean)
is.data.frame(weights_res_mean)
weights_res_mean <- as.data.frame(rowMeans(weights_res))
weights_res_mean$scenario <- rep(data_scen, each = length(model_scen))
weights_res_mean$model <- rep(model_scen, times = length(data_scen))
View(weights_res_mean)
library(tidyverse)
weights_res_mean %>% pivot_wider(names_from = c(scenario, model), values_from = `rowMeans(weights_res)`)
temp <- weights_res_mean %>% pivot_wider(names_from = c(scenario, model), values_from = `rowMeans(weights_res)`)
View(temp)
temp <- weights_res_mean %>% pivot_wider(names_from = c(model), values_from = `rowMeans(weights_res)`)
